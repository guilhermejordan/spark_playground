{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568eb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c81348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade9d067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 10:36:54 WARN Utils: Your hostname, jordan resolves to a loopback address: 127.0.1.1; using 192.168.0.16 instead (on interface wlp0s20f3)\n",
      "23/04/05 10:36:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/jordan/.cache/pypoetry/virtualenvs/spark-playground-X8qIQtop-py3.10/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jordan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jordan/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.3-bundle_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c6254604-2468-421d-a636-dd60743a94c2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3.3-bundle_2.12/0.13.0/hudi-spark3.3-bundle_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.0!hudi-spark3.3-bundle_2.12.jar (18841ms)\n",
      ":: resolution report :: resolve 4130ms :: artifacts dl 18843ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c6254604-2468-421d-a636-dd60743a94c2\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 0 already retrieved (60977kB/49ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 10:37:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 10:37:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/05 10:37:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88eee001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 10:43:41 WARN HoodieBackedTableMetadata: Metadata table was not found at path /home/jordan/spark_playground/data/hudi/rand_nums/.hoodie/metadata\n",
      "23/04/05 10:43:41 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 10:43:50 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_64_1 in memory.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_1 in memory! (computed 0.0 B so far)\n",
      "23/04/05 10:43:50 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_64_11 in memory.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_11 in memory! (computed 0.0 B so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_11 to disk instead.\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_1 to disk instead.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_64_10 in memory.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_10 in memory! (computed 0.0 B so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_10 to disk instead.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_64_6 in memory.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_6 in memory! (computed 0.0 B so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_6 to disk instead.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_64_7 in memory.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_7 in memory! (computed 0.0 B so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_7 to disk instead.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_4 in memory! (computed 1027.2 KiB so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_4 to disk instead.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_9 in memory! (computed 1027.1 KiB so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_9 to disk instead.\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_2 in memory! (computed 1543.0 KiB so far)\n",
      "23/04/05 10:43:50 WARN MemoryStore: Not enough space to cache rdd_64_0 in memory! (computed 1027.0 KiB so far)\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_0 to disk instead.\n",
      "23/04/05 10:43:50 WARN BlockManager: Persisting block rdd_64_2 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 10:44:08 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n",
      "23/04/05 10:44:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "from pyspark.sql.functions import (\n",
    "    rand,\n",
    "    current_timestamp,\n",
    ")\n",
    "\n",
    "# Generate the DataFrame\n",
    "df = (\n",
    "    spark.range(1_000_000)\n",
    "    .withColumn(\"timestamp\", current_timestamp())\n",
    "    .withColumn(\"random_float\", rand(seed=42))\n",
    ")\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": \"rand_nums\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"timestamp\",\n",
    "    # \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    # \"write.insert.cluster\": \"true\",\n",
    "    # \"hoodie.archive.merge.enable\": \"true\",\n",
    "}\n",
    "\n",
    "(\n",
    "    df.write.format(\"org.apache.hudi\")\n",
    "    .options(**hudi_options)\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/home/jordan/spark_playground/data/hudi/rand_nums\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
